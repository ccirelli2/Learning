{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chapter 3 - Starting to Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def write_to_excel(dataframe, filename):\n",
    "    writer = pd.ExcelWriter(filename+'.xlsx')\n",
    "    dataframe.to_excel(writer, 'Data')\n",
    "    writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Url_EEOC = 'https://www.eeoc.gov/eeoc/newsroom/release/index.cfm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Links off Wikipedia Page\n",
    "\n",
    "def get_wiki_href():\n",
    "    html = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')\n",
    "    bsObj = BeautifulSoup(html, 'lxml')\n",
    "    for link in bsObj.findAll('a'):         #Finds all 'a' tags\n",
    "        if 'href' in link.attrs:\n",
    "            print(link.attrs['href'])\n",
    "\n",
    "def get_wiki_tag_a():\n",
    "    html = urlopen('https://www.eeoc.gov/eeoc/newsroom/index.cfm')\n",
    "    bsObj = BeautifulSoup(html, 'lxml')\n",
    "    List_href = []\n",
    "    for x in bsObj.findAll('a'):\n",
    "        if 'href' in x.attrs:\n",
    "            List_href.append(x)\n",
    "    return List_href\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Articles from EEOC Web Page\n",
    "\n",
    "def get_All_Articles_EEOC(Url):\n",
    "    # Create Bs4\n",
    "    html = urlopen(Url)\n",
    "    bsObj = BeautifulSoup(html.read(), 'lxml')\n",
    "    \n",
    "    # Scrape Only News Article Links\n",
    "    Links = bsObj.findAll('a', {'href':re.compile('\\/eeoc\\/newsroom\\/release\\/.*\\.cfm')})\n",
    "    \n",
    "    # Lists\n",
    "    List_href = []\n",
    "    List_complete_direction = []\n",
    "    List_headline = []\n",
    "    Root = 'https://www.eeoc.gov'\n",
    "    \n",
    "    # Get Links\n",
    "    for x in Links:\n",
    "        List_href.append(x.attrs['href'])\n",
    "    \n",
    "    # Concatanate Root and link\n",
    "    for x in List_href:\n",
    "        y = Root + x \n",
    "        List_complete_direction.append(y)\n",
    "    \n",
    "    # Get headline text\n",
    "    for x in Links:\n",
    "        List_headline.append(x.get_text())\n",
    "      \n",
    "    # Create Dataframehead\n",
    "    df = pd.DataFrame(List_complete_direction, List_headline)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Article Text\n",
    "\n",
    "Url_single_article = 'https://www.eeoc.gov/eeoc/newsroom/release/12-1-17.cfm'\n",
    "\n",
    "def get_Article_Text(Url):\n",
    "    # Create Bs4\n",
    "    html = urlopen(Url)\n",
    "    bsObj = BeautifulSoup(html.read(), 'lxml')\n",
    "    Paragraph = bsObj.find('div', {'id':'cs_control_4119'})\n",
    "    \n",
    "    print(Paragraph)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notes\\n\\nRegex        Limits the links to those that contain /wiki/ at the beginning of the link. \\n\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get All Links From Page\n",
    "\n",
    "Url_wiki = urlopen(r'https://en.wikipedia.org/wiki/Python_(programming_language)')\n",
    "Url_EEOC = urlopen(r'https://www.eeoc.gov/')\n",
    "Url_SCA = urlopen(r'http://securities.stanford.edu/filings.html')\n",
    "\n",
    "def get_links_singlePage(Url):\n",
    "    List_links = []\n",
    "    bs4Obj = BeautifulSoup(Url, 'lxml')\n",
    "    for link in bs4Obj.findAll('a', {'href': re.compile('^/wiki/.*')}):  \n",
    "        List_links.append(link['href'])                                  \n",
    "    return List_links                                                    \n",
    "\n",
    "'''Notes\n",
    "\n",
    "Regex        Limits the links to those that contain /wiki/ at the beginning of the link. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HTTPResponse' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-269a943bfd2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m '''\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mgetLinks_entireSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUrl_wiki\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-101-269a943bfd2f>\u001b[0m in \u001b[0;36mgetLinks_entireSite\u001b[1;34m(Url)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetLinks_entireSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mglobal\u001b[0m \u001b[0mpages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mbsObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Chris.Cirelli\\AppData\\Local\\Continuum\\Anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;31m# pre-process request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HTTPResponse' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "# Index Links for an entire site\n",
    "\n",
    "\n",
    "pages = set()\n",
    "\n",
    "def getLinks_entireSite(Url):\n",
    "    global pages \n",
    "    html = urlopen(Url)\n",
    "    bsObj = BeautifulSoup(html)\n",
    "    \n",
    "\n",
    "    for link in bsObj.findAll('a', href = re.compile('^(/wiki/)')):\n",
    "        if 'href' in link.attrs:\n",
    "            print(link.attrs['href'])\n",
    "     \n",
    "'''\n",
    "        if 'href' in link.attrs:                  # Confirm first that its a link\n",
    "            if link.attrs['href'] not in pages:   # Confirm that it is not in set already\n",
    "            # We have encountered a new page\n",
    "                newPage = link.attrs['href']\n",
    "                pages.add(newPage)                    # sets uses add() instead of append()\n",
    "                getLinks(newPage)                     # Recursion\n",
    "'''\n",
    "\n",
    "getLinks_entireSite(Url_wiki)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
